{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb18282-f058-4af1-ba18-29a2f8928620",
   "metadata": {},
   "source": [
    "# One-hotエンコーディング\n",
    "One-hotエンコーディングを行うことでテキストをベクトル化することができる. One-hotエンコーディングによるテキスト表現は直観的で実装が簡単であるが, ベクトルの大きさが語彙数に比例することによってスパースな表現になる, テキストを固定長で表現できない, 単語間の類似性という概念を持たない, 未知の単語に適応できないという問題点がある."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa9c8f48-f0a9-4bf8-82b2-dbb8f1efb3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n",
      "man bites dog\n",
      "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "# 小文字化してピリオドを取り除く\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "\n",
    "# vocabの構築\n",
    "vocab={}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count+1\n",
    "            vocab[word] = count\n",
    "print(vocab)\n",
    "\n",
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded\n",
    "\n",
    "print(processed_docs[1])\n",
    "print(get_onehot_vector(processed_docs[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27f07c-8e91-4c0c-ace7-4add5e8f9557",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "Bag of Wordsはテキストを単語の集合として表現することでテキスト表現を行う手法である. BoWは同じ単語を含むテキストのベクトル表現が近くなるため文書の類似性を捉えているといえる. また任意の長さの文を固定長の符号で表すことができる. 一方でスパース性の問題や, 同じ意味をもつ異なる単語への類似性がないこと, OOVが処理できない, 語順が失われるという問題がある."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee92744d-9f6c-4160-97ee-2824972572bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "dog bites man\n",
      "[[1 1 0 0 1 0]]\n",
      "man bites dog\n",
      "[[1 1 0 0 1 0]]\n",
      "dog eats meat\n",
      "[[0 1 1 0 0 1]]\n",
      "man eats food\n",
      "[[0 0 1 1 1 0]]\n",
      "[[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "# 語彙を表示\n",
    "print(count_vect.vocabulary_)\n",
    "\n",
    "# Bowを表示\n",
    "for i in range(len(processed_docs)):\n",
    "    print(processed_docs[i])\n",
    "    print(bow_rep[i].toarray())\n",
    "    \n",
    "# 新しいテキストに対するBoW\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2d5be-b9e0-424d-b844-951b8b9e04a2",
   "metadata": {},
   "source": [
    "# Bag of N-grams\n",
    "Bag of N-grams(BoN)はテキストを連続するn個の単語に分割することでフレーズや語順を考慮したテキスト表現を作成する方法である. BoNは同じnグラムを含む文書に対する類似性を捉えることができる一方で, nが増加するとスパース性が急速に増加する, OOVが処理できないという問題がある."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0715b362-d691-4afd-9d95-93b283beef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n"
     ]
    }
   ],
   "source": [
    "# n=1,2,3のときのBoN\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "# 語彙を表示\n",
    "print(count_vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f1fb69-4c97-4030-942b-c67efda6b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog bites man\n",
      "[[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "man bites dog\n",
      "[[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "dog eats meat\n",
      "[[0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1]]\n",
      "man eats food\n",
      "[[0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 1 0]]\n",
      "[[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 新しいテキストに対するBoW\n",
    "\n",
    "# BoNを表示\n",
    "for i in range(len(processed_docs)):\n",
    "    print(processed_docs[i])\n",
    "    print(bow_rep[i].toarray())\n",
    "\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae5cca-80e7-42ed-a1d8-cf5879ec8d69",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "TF-IDFはある文書中に登場する単語の頻度TFと, 文書間である単語の頻度を比較するIDFの積で表されるテキスト表現である. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701c22e1-ff46-47ae-9dfb-7f492adc77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "dog bites man\n",
      "[[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]]\n",
      "man bites dog\n",
      "[[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]]\n",
      "dog eats meat\n",
      "[[0.         0.44809973 0.55349232 0.         0.         0.70203482]]\n",
      "man eats food\n",
      "[[0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "<bound method CountVectorizer.get_feature_names of TfidfVectorizer()>\n",
      "[1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "[[0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "# 語彙を表示\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# TF-IDFを表示\n",
    "for i in range(len(processed_docs)):\n",
    "    print(processed_docs[i])\n",
    "    print(bow_rep_tfidf[i].toarray())\n",
    "\n",
    "print(tfidf.get_feature_names) # 全単語\n",
    "print(tfidf.idf_) # IDF\n",
    "\n",
    "temp = tfidf.transform([\"dog and dog are friends\"])\n",
    "print(temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ff8da-ea99-49c8-8c27-125cedfc7d3b",
   "metadata": {},
   "source": [
    "## 事前学習済み単語埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d82e4e7-4f6a-4e7f-a741-d9e9faaaab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-04 10:07:26--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.37.86\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.37.86|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  11.0MB/s    in 2m 42s  \n",
      "\n",
      "2022-03-04 10:10:09 (9.71 MB/s) - ‘/tmp/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 事前学習済み単語埋め込みのダウンロード\n",
    "!wget -P /tmp/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf910efa-2776-4716-8072-28e53b51bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "\n",
    "path = \"/tmp/input/GoogleNews-vectors-negative300.bin.gz\"\n",
    "vectors = KeyedVectors.load_word2vec_format(path,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d49377-4b3d-4daa-aa8e-77fa1f375351",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# beautifulに類似する単語を取得\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvectors\u001b[49m\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeautiful\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "# beautifulに類似する単語を取得\n",
    "print(vectors.most_similar(\"beautiful\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27694142-0e23-4b42-8ed4-b38df99642cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
