{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b75f1a-a9a2-4af1-be2a-db34a1403e8c",
   "metadata": {},
   "source": [
    "# CNN, RNNを用いた対話行為の分類\n",
    "\n",
    "CNNまたはRNNを用いて対話行為の分類を行う. データセットはATIS(Aierline Travel Information Systems):航空券予約のデータセットを用いる. このデータセットは4478個の学習用音声と893個のテスト用音声からなり, 21個の意図が含まれている. 学習にはこのうち17個の意図を用いる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3f05d1d-db65-47b6-b5e8-e4371672e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import (LSTM, Conv1D, Dense, Embedding, GlobalMaxPooling1D, MaxPooling1D, TextVectorization)\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3ce4d-b3c0-4dd2-a5c7-d1e899383d3c",
   "metadata": {},
   "source": [
    "## データセット, モデルの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048391f7-50e7-481c-9607-cf33c254e3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS i want to fly from boston at 838 am and arrive in denver at 1110 in the morning EOS\t O O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day atis_flight\n",
      "BOS what flights are available from pittsburgh to baltimore on thursday morning EOS\tO O O O O O B-fromloc.city_name O B-toloc.city_name O B-depart_date.day_name B-depart_time.period_of_day atis_flight\n",
      "BOS what is the arrival time in san francisco for the 755 am flight leaving washington EOS\tO O O O B-flight_time I-flight_time O B-fromloc.city_name I-fromloc.city_name O O B-depart_time.time I-depart_time.time O O B-fromloc.city_name atis_flight_time\n",
      "BOS cheapest airfare from tacoma to orlando EOS\tO B-cost_relative O O B-fromloc.city_name O B-toloc.city_name atis_airfare\n",
      "BOS round trip fares from pittsburgh to philadelphia under 1000 dollars EOS\tO B-round_trip I-round_trip O O B-fromloc.city_name O B-toloc.city_name B-cost_relative B-fare_amount I-fare_amount atis_airfare\n",
      "BOS i need a flight tomorrow from columbus to minneapolis EOS\tO O O O O B-depart_date.today_relative O B-fromloc.city_name O B-toloc.city_name atis_flight\n",
      "BOS what kind of aircraft is used on a flight from cleveland to dallas EOS\tO O O O O O O O O O O B-fromloc.city_name O B-toloc.city_name atis_aircraft\n",
      "BOS show me the flights from pittsburgh to los angeles on thursday EOS\tO O O O O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O B-depart_date.day_name atis_flight\n",
      "BOS all flights from boston to washington EOS\tO O O O B-fromloc.city_name O B-toloc.city_name atis_flight\n",
      "BOS what kind of ground transportation is available in denver EOS\tO O O O O O O O O B-city_name atis_ground_service\n"
     ]
    }
   ],
   "source": [
    "# データセットとテストセットの先頭行, 形式が異なるため注意が必要\n",
    "\n",
    "!head ./data/atis.train.w-intent.iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fad3da-b8b4-4de8-9098-b1230fc956b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS O\n",
      "i O\n",
      "would O\n",
      "like O\n",
      "to O\n",
      "find O\n",
      "a O\n",
      "flight O\n",
      "from O\n",
      "charlotte B-fromloc.city_name\n"
     ]
    }
   ],
   "source": [
    "!head ./data/atis.test.w-intent.iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773c9d21-793f-4657-893c-1475c6bd8c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-09 15:53:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-04-09 15:53:12--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  4.46MB/s    in 2m 42s  \n",
      "\n",
      "2022-04-09 15:55:54 (5.08 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: data/glove.6B.50d.txt   \n",
      "  inflating: data/glove.6B.100d.txt  \n",
      "  inflating: data/glove.6B.200d.txt  \n",
      "  inflating: data/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "# GloVe(学習済み単語埋め込み)のダウンロードと展開\n",
    "!wget  https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51ab7e3-53b4-4142-afaa-2c386b4449a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./data/atis.train.w-intent.iob\"\n",
    "test_data_path = \"./data/atis.test.w-intent.iob\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c523f7-4ec7-4822-bfb6-c89a120332a4",
   "metadata": {},
   "source": [
    "次の関数ではtrainデータを読み込んで文, ラベル(対話行為の分類ラベル), 意図の3つに分割してリストに格納する処理を行う. 例えばtrain1行目のデータ\n",
    "```\n",
    "BOS i want to fly from boston at 838 am and arrive in denver at 1110 in the morning EOS\t O O O O O O B-fromloc.city_name O B-depart_time.time I-depart_time.time O O O B-toloc.city_name O B-arrive_time.time O O B-arrive_time.period_of_day atis_flight\n",
    "```\n",
    "の場合はBOSからEOSまでが文でそれ以降がラベルである. そして末尾の要素は意図を表す. IやwantはO(固有表現でないことを表すラベル), bostonにはB-fromloc.city_name(街の名前)を表すラベルというように固有表現の種類ごとにラベル付けされいる. 意図はatis_○○という形式であるから先頭のatis_を削除して格納する. このようなデータを次の3つのリストsents,labels,intentsに格納する. \n",
    "```python \n",
    "sents = ['BOS', 'i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning', 'EOS'] \n",
    "labels = ['', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-depart_time.time', 'I-depart_time.time', 'O', 'O', 'O', 'B-toloc.city_name', 'O', 'B-arrive_time.time', 'O', 'O', 'B-arrive_time.period_of_day']\n",
    "intents[0] = 'atis_flight'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f326f8-7d44-4bfa-aa18-c228c04b9ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(filename,remove_validation=True):\n",
    "    sents, labels, intents = [], [], []\n",
    "    with open(filename,encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            words, labs = [i.split(' ') for i in line.strip().split('\\t')]\n",
    "            if remove_validation and \"#\" in labs[-1]:\n",
    "                continue\n",
    "            sents.append(words[1:-1])\n",
    "            labels.append(labs[1:-1])\n",
    "            intents.append(re.sub(r\"^atis_\", \"\", labs[-1]))\n",
    "    return sents, labels, intents           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994e3f1c-17c9-4ad5-b0e3-550d96183633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences : 4952\n",
      "Number of unique intents : 17\n",
      "('i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'flight')\n",
      "('what flights are available from pittsburgh to baltimore on thursday morning', 'flight')\n",
      "('what is the arrival time in san francisco for the 755 am flight leaving washington', 'flight_time')\n",
      "('cheapest airfare from tacoma to orlando', 'airfare')\n",
      "('round trip fares from pittsburgh to philadelphia under 1000 dollars', 'airfare')\n"
     ]
    }
   ],
   "source": [
    "# load train\n",
    "sents, labels, intents = load_train_data(train_data_path)\n",
    "train_texts = [\" \".join(words)for words in sents]\n",
    "train_labels = intents\n",
    "\n",
    "print(\"Number of training sentences :\", len(train_texts))\n",
    "print(\"Number of unique intents :\", len(set(train_labels)))\n",
    "for i in zip(train_texts[:5], train_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f126c906-156a-4cba-88d7-522ceb06976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(filename, remove_validation=True):\n",
    "    sents, labels, intents = [], [], []\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        words, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, tag = line.split()\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                if not (remove_validation and \"#\" in tags[-1]):\n",
    "                    sents.append(words[1: -1])\n",
    "                    labels.append(tags[1: -1])\n",
    "                    intents.append(re.sub(r\"^atis_\", \"\", tags[-1]))\n",
    "                words, tags = [], []\n",
    "    return sents, labels, intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e16ed684-c35a-4d94-a35b-439d3a6ecbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day_name\n",
      "day_name\n",
      "Number of testing sentences : 876\n",
      "Number of unique intents : 15\n",
      "('i would like to find a flight from charlotte to las vegas that makes a stop in st. louis', 'flight')\n",
      "('on april first i need a ticket from tacoma to san jose departing before 7 am', 'airfare')\n",
      "('on april first i need a flight going from phoenix to san diego', 'flight')\n",
      "('i would like a flight traveling one way from phoenix to san diego on april first', 'flight')\n",
      "('i would like a flight from orlando to salt lake city for april first on delta airlines', 'flight')\n"
     ]
    }
   ],
   "source": [
    "sents, labels, intents = load_test_data(test_data_path)\n",
    "\n",
    "test_texts = [\" \".join(words) for words in sents]\n",
    "test_labels = intents\n",
    "\n",
    "new_labels = set(test_labels) - set(train_labels)\n",
    "\n",
    "# テストデータにだけ出現するラベルを除去\n",
    "vals = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] in new_labels:\n",
    "        print(test_labels[i])\n",
    "        vals.append(i)\n",
    "for i in vals[::-1]:\n",
    "    test_labels.pop(i)\n",
    "    test_texts.pop(i)\n",
    "\n",
    "print(\"Number of testing sentences :\", len(test_texts))\n",
    "print(\"Number of unique intents :\", len(set(test_labels)))\n",
    "for i in zip(test_texts[:5], test_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14820ad-85c0-4abe-987e-3dc329b524d5",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db1a3765-eff8-47d3-b4d4-e5ce0d1558d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"data\"\n",
    "GLOVE_PATH = os.path.join(BASE_DIR,\"glove.6B.100d.txt\")\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9e1498b-233c-4140-b6d2-d5d2e7831eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 16:34:54.239045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:54.305979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:54.306206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:54.307539: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-09 16:34:54.308337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:54.308555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:54.308822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:55.402651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:55.402889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:55.402905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-04-09 16:34:55.403126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-09 16:34:55.403639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3929 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:0a:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語をIDに変換\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens = MAX_NUM_WORDS,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "vectorize_layer.adapt(train_texts)\n",
    "vectorize_layer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee387800-eddf-4b69-930a-fa07741406b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_layer(train_texts).numpy()\n",
    "x_test = vectorize_layer(test_texts).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b626e58e-be7c-4084-adbc-7157d7a49147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9,  9, 11,  2,  2,  9,  1,  9,  9, 13])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ラベルをIDに変換\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "y_train = le.transform(train_labels)\n",
    "y_test = le.transform(test_labels)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77d5fd39-219b-4371-bdfa-fb583b8a6089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flight', 'flight', 'flight_time', 'airfare', 'airfare']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45d8f566-2098-4663-a75a-40ea2d9db3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8aed62e-7dab-44e0-af56-053a3b58f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors in Glove embeddings.\n"
     ]
    }
   ],
   "source": [
    "# 埋め込み行列の準備\n",
    "# 最初に、単語のインデックスとベクトルのマッピングを作成\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_PATH)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "\n",
    "# 埋め込み行列の準備\n",
    "# 行は単語、列はGloVeから得た埋め込みに対応\n",
    "num_words = min(MAX_NUM_WORDS, vectorize_layer.vocabulary_size()) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # 単語が見つからなければ、ゼロベクトルのまま\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Embedding層に事前学習済み単語埋め込みを読み込み\n",
    "# 埋め込みを更新しないように、trainable=Falseを設定していることに注意\n",
    "embedding_layer = Embedding(\n",
    "    num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0dc8f-baa3-451c-a2a2-42dfa59d2678",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64e953-a877-4314-8c43-80955c24cd47",
   "metadata": {},
   "source": [
    "### 事前学習済み埋め込みを用いたCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fcc385c-46c4-4329-9e5c-80d89c1b5100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          89700     \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 296, 128)          64128     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 59, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 55, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 11, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 7, 128)            82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17)                2193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,629\n",
      "Trainable params: 246,929\n",
      "Non-trainable params: 89,700\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 16:46:06.407907: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 10s 27ms/step - loss: 1.3489 - acc: 0.7112 - val_loss: 0.9677 - val_acc: 0.7443\n",
      "28/28 [==============================] - 0s 5ms/step - loss: 1.0820 - acc: 0.7215\n",
      "Test accuracy with CNN: 0.7214611768722534\n"
     ]
    }
   ],
   "source": [
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation=\"relu\"))\n",
    "cnnmodel.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "\n",
    "cnnmodel.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "cnnmodel.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")\n",
    "score, acc = cnnmodel.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy with CNN:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a169f95-75fa-4417-b03f-6b88fab95fcc",
   "metadata": {},
   "source": [
    "### 事前学習済み埋め込みを用いないCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa585f63-2b22-487a-8183-63e0339b7aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 17)                2193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,824,849\n",
      "Trainable params: 2,824,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 1.5724 - acc: 0.7164 - val_loss: 1.0670 - val_acc: 0.7443\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 1.2541 - acc: 0.7215\n",
      "Test accuracy with CNN: 0.7214611768722534\n"
     ]
    }
   ],
   "source": [
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128, mask_zero=True))\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation=\"relu\"))\n",
    "cnnmodel.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "\n",
    "cnnmodel.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "cnnmodel.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")\n",
    "score, acc = cnnmodel.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy with CNN:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74286dda-949a-4cfc-bd87-63485d6b1c38",
   "metadata": {},
   "source": [
    "### 事前学習済み埋め込みを用いたLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80667c08-e265-41ad-8680-d11026b58ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          89700     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 17)                2193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 209,141\n",
      "Trainable params: 119,441\n",
      "Non-trainable params: 89,700\n",
      "_________________________________________________________________\n",
      "109/109 [==============================] - 337s 3s/step - loss: 0.9672 - accuracy: 0.7582 - val_loss: 0.5785 - val_accuracy: 0.8567\n",
      "28/28 [==============================] - 4s 128ms/step - loss: 0.7192 - accuracy: 0.8162\n",
      "Test accuracy with RNN: 0.8162100315093994\n"
     ]
    }
   ],
   "source": [
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "rnnmodel2.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "rnnmodel2.summary()\n",
    "\n",
    "rnnmodel2.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")\n",
    "score, acc = rnnmodel2.evaluate(x_test, y_test, batch_size=32)\n",
    "print(\"Test accuracy with RNN:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165d1d4-d167-4ffd-987d-b33b87546441",
   "metadata": {},
   "source": [
    "### 事前学習済み埋め込みを用いないLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca3f803f-333d-4bf6-88a6-aada93916be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 17)                2193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,693,777\n",
      "Trainable params: 2,693,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "109/109 [==============================] - 353s 3s/step - loss: 1.1637 - accuracy: 0.7530 - val_loss: 0.8114 - val_accuracy: 0.7840\n",
      "28/28 [==============================] - 4s 125ms/step - loss: 0.9187 - accuracy: 0.7557\n",
      "Test accuracy with RNN: 0.7557077407836914\n"
     ]
    }
   ],
   "source": [
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128, mask_zero=True))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "rnnmodel.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "rnnmodel.summary()\n",
    "\n",
    "rnnmodel.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=1,\n",
    "    validation_data=(x_valid, y_valid)\n",
    ")\n",
    "score, acc = rnnmodel.evaluate(x_test, y_test, batch_size=32)\n",
    "print(\"Test accuracy with RNN:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cb281-8c9e-4741-b16a-ade1db5de0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
