{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c04f3d-ea71-46fd-8557-5d3abea02069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punkt,stopwords,wordnet,omwのダウンロード\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b4bfb-1b96-4cc5-a726-2bd4deaa562b",
   "metadata": {},
   "source": [
    "# 前処理\n",
    "\n",
    "本プログラムでは英語の文章に対する前処理を対象とする."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbe284-794f-4a83-88e2-8d215d6f29e5",
   "metadata": {},
   "source": [
    "## 文の分割\n",
    "\n",
    "まず, 文章を1文ずつに分割する処理を行う. 英語の文章はおおよそピリオド(.)とクエスチョンマーク(?)で1文ずつに分割することができる. しかし敬称(Dr. Mr.)や省略形(...)の例外が存在する. ここでは文章を1文ずつに分割する処理を行うためライブラリであるnltkライブラリを用いて処理を行う."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d1142e-5d3d-4d03-9fb9-fa9b64288831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!.',\n",
       " 'It should be done by the ending of this month.',\n",
       " 'But will it?',\n",
       " 'This notebook has been run 4 times !',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "# 文に分割\n",
    "mytext = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "my_sentences = sent_tokenize(mytext)\n",
    "my_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8481fe-44aa-4802-9222-07c8867d307d",
   "metadata": {},
   "source": [
    "## 単語への分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9782267a-7676-45c4-8ff4-0b994d96d0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!.\n",
      "['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', '!', '!', '.']\n",
      "It should be done by the ending of this month.\n",
      "['It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', '.']\n",
      "But will it?\n",
      "['But', 'will', 'it', '?']\n",
      "This notebook has been run 4 times !\n",
      "['This', 'notebook', 'has', 'been', 'run', '4', 'times', '!']\n",
      "!\n",
      "['!']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbeaba9-7e17-49ca-acf5-f5af9e3dbbba",
   "metadata": {},
   "source": [
    "## ストップワードの処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767bdcb-6ee4-407e-b580-8f555e06a96b",
   "metadata": {},
   "source": [
    "ある記事を単語に分割したものを用いて, その記事が政治, スポーツ, ビジネス, その他のどれに分類されるかを判定する分類器を作成するタスクを考える. 単語の中には「a」,「the」,「on」のように英語で頻繁に用いられる単語がある. これらの単語は分類器の作成に寄与しないストップワードと呼ばれ, 前処理の段階で除外される. さらに例にあげたタスクは記事を対象としているため「news」という単語もストップワードになりえる. このように一般にはストップワードではないが, 扱っているコーパスが特定分野のものであるときにはその分野で頻出する単語もストップワードになる. ここではnltkのストップワードを用いて除外を行う."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe52b569-edb0-4815-bf24-9d10f3f2bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stopwordsを表示\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3903fd8f-bb71-463b-a5fe-7bdfcc1ae623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(texts):\n",
    "    \"\"\"文を単語に分割してストップワードを除外する関数\n",
    "    \n",
    "    Args:\n",
    "    texts : list of texts\n",
    "    \n",
    "    Returns:\n",
    "    list of words with stop words excluded\n",
    "    \"\"\"\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    def remove_stops_digits(tokens):\n",
    "        # lower() : すべての文字を小文字にする\n",
    "        # isdigit() : 文字列が数字か英字か, true->数字, false->英字\n",
    "        # punctuation : 句読点のリスト\n",
    "        return [token.lower() for token in tokens if token not in mystopwords\n",
    "               and not token.isdigit() and token not in punctuation]\n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6eb2067-dfe4-451d-aa9a-0a9c92b2bc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon'],\n",
       " ['it', 'done', 'ending', 'month'],\n",
       " ['but'],\n",
       " ['this', 'notebook', 'run', 'times'],\n",
       " []]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_corpus(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdaf778-cc46-4cf5-b603-4f7fdb37432a",
   "metadata": {},
   "source": [
    "## ステミング\n",
    "ステミングは接頭辞を取り除いて単語を基本形に直す処理を行うことである. 例えば「car」と「cars」という単語はどちらもステミングすると「car」という単語になる.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fda84155-1080-4747-a289-5595a16cde83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "revolut\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"cars\")) # ステミングが適切な例\n",
    "print(stemmer.stem(\"revolution\")) # ステミングが適切でない例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396860b1-24da-4c49-8857-311f8fb0061c",
   "metadata": {},
   "source": [
    "## 見出し語化\n",
    "見出し語化は単語を基本となる単語(見出し語)に直す処理を行うことである. 定義はステミングに近いが内容は異なる. 単語「better」はステミングでは「better」のままであるが見出し語化を行うと「good」になる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7e7d46b-edaf-4869-a43a-578b5a7129ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(stemmer.stem(\"better\")) # ステミング\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\")) # 見出し語化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147c4d5-1e7e-4521-aec8-c12807cc898d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
