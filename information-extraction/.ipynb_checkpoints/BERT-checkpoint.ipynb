{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16983e05-b2c8-4033-8c3b-caa9d42f5f0c",
   "metadata": {},
   "source": [
    "# BERTを用いた固有表現認識\n",
    "BERT(Bidirectional Encoder Representations from Transformers)を用いて固有表現認識を行う. BERTはPre-Trainingとfine-tuningの仕組みによって誰でも簡単に用いることができるために流行したモデルである. データセットはCoNLL2003を用いる."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed2ee4-4f0c-475d-8dd1-0439bf089e33",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a875bc60-4599-4fed-85ba-3c6e6d9cf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aae794f-2df9-4829-b79a-e7f15941bf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aea9d-e71e-4f9b-9e78-646376f6abed",
   "metadata": {},
   "source": [
    "# データセット読み込み\n",
    "datasetsライブラリを用いてCoNLL2003データセットを読み込む. ラベルが既にKey-Value形式で与えられているからモデルで簡単に扱うことができる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb0a97f-bb8a-425b-8b63-fa0466dac4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
      "100% 3/3 [00:00<00:00, 541.71it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230c96c2-6b10-42ae-8e04-abd9449f0c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14042\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3251\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3454\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55cf6da-5d09-4467-9eb1-6d0a4155dbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d37781-f548-44ce-be03-b11cad4c0f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47857dd-896f-40db-8190-da2a0a836f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9afec6-52ff-49ec-b502-4afd8f585aac",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c33dc3-dcad-4835-8ae1-fc72c5ea0068",
   "metadata": {},
   "source": [
    "BERTに入力するためにテキストをトークン化してモデルが期待するフォーマットに変換する. 次のセルでは使用するモデルのアーキテクチャに対応したtokenizerの取得と, Pre-Trainingに使用した語彙をダウンロードする."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8f346b-18b7-424a-a8b5-9556feeb483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395ee68c-a5da-4a0a-b917-b57883f4d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizerの使用例\n",
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064f06db-c267-4bc1-b806-dcc6c5f67878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'this', 'is', 'one', 'sentence!', 'split', 'into', 'words', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 3975, 2046, 2616, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力単語が分割済みの場合\n",
    "split_words = \"Hello, this is one sentence! split into words .\".split()\n",
    "print(split_words)\n",
    "tokenizer(split_words,is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5b20743-7ac4-4fa9-82ea-92e166c7c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.'] \n",
      "\n",
      "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# サブワードレベルに分割する場合\n",
    "# trainから適当にテキストを抽出\n",
    "example = datasets[\"train\"][4]\n",
    "print(example[\"tokens\"],\"\\n\")\n",
    "\n",
    "# サブワードレベルのtokenizerを使用\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3285d3-024d-4a3b-8c7b-af5282044a1f",
   "metadata": {},
   "source": [
    "sheepmeatという単語が'sheep','me','at'というサブワードに分割されている.\n",
    "\n",
    "tokenizerの処理によって[CLS]や[SEP]のように文頭や2文目との区切りを表す特殊文字が追加されるため入力と出力の長さが一致しなくなる問題がある. またサブワードレベルに分割した場合も入力と出力の長さが一致しなくなる. しかしtokenizerのward_idsは1つの単語から生成されたサブワードに同じidsを振るためサブワードに分割したときについては問題ない."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51eea0f-6969-4388-a40a-83ff54666fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 39)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[\"ner_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b8610b-013a-4288-be2a-c6958c4bc533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5090fc8e-769b-4188-9c9d-a85b68661eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -100]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特殊なトークンをNone, その他のトークンをそれぞれの単語にマッピングすることでサブワードと元の単語の対応付けを行ってかつ, 文字列の長さを同じにする\n",
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"ner_tags\"][i] for i in word_ids]\n",
    "aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125f0acc-e108-4f89-a90b-8c9c51f6a829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    }
   ],
   "source": [
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabc066-2839-4a66-bbd4-cf6bf3c8c5ed",
   "metadata": {},
   "source": [
    "ここでは特殊なトークンに-100(pytorchが無視する値), 他のすべてのトークンに対して元となった単語のラベルを設定している.\n",
    "別の方法としてはある単語から得られた最初のトークンにのみラベルを設定し, 同じ単語から得られた他のサブトークンには-100のラベルを与えるという方法もある. 方法を切り替えられるようにフラグを準備しておく."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476d322a-19a0-4be0-87b5-4696441be03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b64473-3b8a-4e4b-a7bf-e4c7d6479c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # 特殊なトークンの場合-100を設定\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # 各単語の最初のトークンにはラベルを設定\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # 残りのトークンは戦略によって、-100か最初のトークンと同じラベルを設定するか切り替える\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e493b50-9957-45ea-bd36-0e8f02feb0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cef398c6-3701-4acc-8a78-0f03c3d6954d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-efe49fb60bb4ec73.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-234ea76de4e2f826.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-67c116316e0ca1b8.arrow\n"
     ]
    }
   ],
   "source": [
    "# datasetにtokenizerを適用\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21cfd972-b0bd-41c7-8b96-93aa942f2a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [101,\n",
       "  7327,\n",
       "  19164,\n",
       "  2446,\n",
       "  2655,\n",
       "  2000,\n",
       "  17757,\n",
       "  2329,\n",
       "  12559,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c839ce5-125d-4f23-8572-1d7ba33cae46",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc86ce25-2700-4c6a-9763-4081c0737970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Pre-trained modelを読み込み\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint,num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2388d00d-8990-4b92-b415-bed6054bb4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "batch_size = 8 #自分の環境では16ではGPUメモリが足りないためエラーが出る\n",
    "args = TrainingArguments(\n",
    "    \"ner-conll2003\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29414f16-621d-4780-ad96-42dfac8ccc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCollatorを用いてデータ長が同じになるようにパディングする\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d007e44e-2ee2-4aa5-9554-6b6f3bf12537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系列ラベリングの評価メトリクス\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a0b205-10c9-4af3-8e7a-b90899fb5d07",
   "metadata": {},
   "source": [
    "metricは予測ラベルと正解ラベルを受け取る. そのため予測値に後処理を行う関数を定義する. 関数の処理内容は次の通りである.\n",
    "- 各トークンの予測インデックス(最大ロジット)を選択し文字列ラベルに変換する.\n",
    "- ラベルに-100を設定している場合は無視する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d6ea6ba-7d3f-4e6c-b39c-e8526ee86a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "740e6bd1-1ebe-408a-8b6c-ce9929b4adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb85f5f-79e7-44de-b119-ee1d210fa1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, chunk_tags, pos_tags, tokens.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14042\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5268\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 09:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.064508</td>\n",
       "      <td>0.929916</td>\n",
       "      <td>0.933661</td>\n",
       "      <td>0.931785</td>\n",
       "      <td>0.983367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.057946</td>\n",
       "      <td>0.938646</td>\n",
       "      <td>0.944737</td>\n",
       "      <td>0.941682</td>\n",
       "      <td>0.986036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.062174</td>\n",
       "      <td>0.935751</td>\n",
       "      <td>0.946638</td>\n",
       "      <td>0.941163</td>\n",
       "      <td>0.986227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ner-conll2003/checkpoint-500\n",
      "Configuration saved in ner-conll2003/checkpoint-500/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-1000\n",
      "Configuration saved in ner-conll2003/checkpoint-1000/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-1500\n",
      "Configuration saved in ner-conll2003/checkpoint-1500/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, chunk_tags, pos_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-2000\n",
      "Configuration saved in ner-conll2003/checkpoint-2000/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-2500\n",
      "Configuration saved in ner-conll2003/checkpoint-2500/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-3000\n",
      "Configuration saved in ner-conll2003/checkpoint-3000/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-3500\n",
      "Configuration saved in ner-conll2003/checkpoint-3500/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, chunk_tags, pos_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-4000\n",
      "Configuration saved in ner-conll2003/checkpoint-4000/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-4500\n",
      "Configuration saved in ner-conll2003/checkpoint-4500/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ner-conll2003/checkpoint-5000\n",
      "Configuration saved in ner-conll2003/checkpoint-5000/config.json\n",
      "Model weights saved in ner-conll2003/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ner-conll2003/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ner-conll2003/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, chunk_tags, pos_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3251\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.06592675475523883, metrics={'train_runtime': 577.9548, 'train_samples_per_second': 72.888, 'train_steps_per_second': 9.115, 'total_flos': 893043283669020.0, 'train_loss': 0.06592675475523883, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b6a444-b582-4b16-9e9e-29205b83dfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: ner_tags, id, chunk_tags, pos_tags, tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3454\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8984517304189436,\n",
       "  'recall': 0.9289077212806026,\n",
       "  'f1': 0.913425925925926,\n",
       "  'number': 2124},\n",
       " 'MISC': {'precision': 0.7821466524973433,\n",
       "  'recall': 0.7389558232931727,\n",
       "  'f1': 0.7599380485286527,\n",
       "  'number': 996},\n",
       " 'ORG': {'precision': 0.8673929376408716,\n",
       "  'recall': 0.8921947449768161,\n",
       "  'f1': 0.8796190476190477,\n",
       "  'number': 2588},\n",
       " 'PER': {'precision': 0.9706755753526355,\n",
       "  'recall': 0.9621044885945548,\n",
       "  'f1': 0.966371027346637,\n",
       "  'number': 2718},\n",
       " 'overall_precision': 0.8987401389379489,\n",
       " 'overall_recall': 0.9058865416567766,\n",
       " 'overall_f1': 0.9022991902594716,\n",
       " 'overall_accuracy': 0.9769870865271736}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ed082f5-f768-4d8c-90e3-4a4542f3bbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3749f12-f35f-441d-a450-3a4151088ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100,    0,    0,    5,    0,    0,    0,    0,    1,    0,    0,\n",
       "          0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f195bed1-2e78-4098-8fa9-6cb32d757017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOCCER',\n",
       " '-',\n",
       " 'JAPAN',\n",
       " 'GET',\n",
       " 'LUCKY',\n",
       " 'WIN',\n",
       " ',',\n",
       " 'CHINA',\n",
       " 'IN',\n",
       " 'SURPRISE',\n",
       " 'DEFEAT',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"test\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f2cf9-2bbe-4c0d-b846-6cce8a75fd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
